Big picture: the plan is impressively aligned with the Python SDK, but there *are* a few real discrepancies (including some that will change behavior vs Python) and a bunch of “doc drift” where older snippets survived later corrections.

I’ll go through the main ones by theme so you can fix them systematically.

---

## 1. Wire formats & type-level inconsistencies

### 1.1 `RequestErrorCategory` – over-cautious / contradictory

You correctly show:

```python
class RequestErrorCategory(StrEnum):
    Unknown = auto()
    Server = auto()
    User = auto()
```

Given Python’s `enum.StrEnum`, `auto()` here will be `"Unknown"`, `"Server"`, `"User"` – deterministically. There’s no real “runtime unknown” about the casing.

But in `01_type_system.md` you still have a “⚠️ REQUIRES RUNTIME VERIFICATION” section suggesting the wire format might be lowercase (`"unknown"`, `"server"`, `"user"`). Later docs (e.g. `05_error_handling.md`) assume capitalized values.

**Impact / fix:**

* Drop the “wire format depends on StrEnum implementation” warning; commit to `"Unknown" | "Server" | "User"`.
* Keep your *defensive* parser (accepting both cases) if you like, but don’t treat the casing as unknown – that will just confuse future you.

---

### 1.2 `SampleRequest.prompt_logprobs` default – one stale struct

You correctly state and mirror the Python type:

```python
prompt_logprobs: Optional[bool] = None
```

and in Elixir:

```elixir
prompt_logprobs: nil  # tri-state: nil | true | false
```

But later, in the JSON encoding section, your “Approach 1: natural nil → null” example uses:

```elixir
defmodule Tinkex.Types.SampleRequest do
  @derive {Jason.Encoder, only: [...]}

  defstruct [
    :sampling_session_id,
    :seq_id,
    :base_model,
    :model_path,
    :prompt,
    :sampling_params,
    num_samples: 1,
    prompt_logprobs: false,   # <-- stale; conflicts with earlier correction
    topk_prompt_logprobs: 0
  ]
end
```

That silently reverts to `false` as default.

**Impact:**
You lose the “not set vs explicitly false” distinction you just spent a whole section preserving, and Elixir no longer matches Python’s behavior.

**Fix:**
Change that example to `prompt_logprobs: nil` and explicitly note that callers should opt in with `true` / `false` while defaulting to `nil`.

---

### 1.3 `TryAgainResponse` / queue state – `type` vs `status`

Your types:

```python
class TryAgainResponse(BaseModel):
    type: Literal["try_again"] = "try_again"
    request_id: str
    queue_state: Literal["active", "paused_capacity", "paused_rate_limit"]
```

So the JSON you get from `/future/retrieve` for this case is shaped like:

```json
{"type": "try_again", "request_id": "...", "queue_state": "paused_rate_limit"}
```

In `03_async_model.md` / `Tinkex.Future.poll/2`, you’re matching on:

```elixir
{:ok, %{status: "try_again", queue_state: queue_state}} -> ...
```

That doesn’t match the Python type definition and will never trigger unless the server also adds a `status` field.

**Fix:**

* Match on `type` rather than `status`, e.g.:

  ```elixir
  {:ok, %{"type" => "try_again", "queue_state" => queue_state}} -> ...
  ```

* Or, if you plan to decode into a typed struct, mirror the Python `TryAgainResponse` and pattern-match on `response.type`.

---

### 1.4 Metrics reduction – claims vs actual algorithm

You say you now “match Python’s `chunked_fwdbwd_helpers._metrics_reduction` exactly”.

Python’s implementation (from `chunked_fwdbwd_helpers.py`) roughly does:

* `keys = results[0].metrics.keys()` – only keys present in the first result.
* For each such `key`:

  * Split suffix (`":mean"`, `":sum"`, `":min"`, `":max"`, `":slack"`, `":unique"`).
  * Use `REDUCE_MAP[suffix]`.
  * Use *weights* = `len(m.loss_fn_outputs)` per result.

Your Elixir `Tinkex.MetricsReduction.reduce/1` does:

* `keys` = union of all metric keys across results.
* For missing metrics, you effectively treat them as `0.0` (`Map.get(..., 0.0)`).
* `:slack` is currently just `reduce_mean/3`.
* `:unique` returns the **first** value.

Those are not guaranteed to match the Python semantics:

* Python only reduces metrics present in `results[0]`. If subsequent chunks add new metric names, Python ignores them; you keep them (with zero fill).
* The comments in `_unique`’s docstring say it uses a hack that adds “unique keys with suffix” rather than collapsing to a single float; your “first value wins” is an approximation.
* Without seeing `_slack`’s implementation, assuming it’s just mean might be wrong.

**Recommendation:**

* Either:

  * Mirror Python literally (including its quirks: `keys = results[0].metrics.keys()` and the exact `_slack` / `_unique` behavior), **or**
  * Document clearly that you *intentionally* diverge, and remove the “matches exactly” language.
* At minimum, don’t silently union keys and inject zeros – that can subtly change metrics.

---

## 2. HTTP / retry behavior

### 2.1 Retry-After – partial parity vs Python

Python’s `_parse_retry_after_header` in `_base_client.py` supports:

* `retry-after-ms` → milliseconds
* `retry-after` → integer or float seconds
* `retry-after` → HTTP-date (IMF-fixdate) via `email.utils.parsedate_tz` + `mktime_tz`

Your Elixir `parse_retry_after/1`:

* Parses `retry-after-ms` (ms) ✅
* Parses `retry-after` as integer seconds ✅
* If `retry-after` is a date string → falls back to `1000` ms with a “TODO v2.0” comment ❌

Yet in the header of `04_http_layer.md` you still have a “Round 6 Verification” bullet claiming HTTP Date parsing is implemented.

**Impact:**
If the server ever uses date-based `Retry-After`, the Python client will back off properly; Elixir will hammer after 1s.

**Fix:**

* Either implement HTTP-date parsing (e.g. `:calendar` + `:httpdate` or a small parser), or explicitly document that v1.0 only honors numeric Retry-After and may be more aggressive than Python.
* Update the Round 6 summary to match the Round 7 reality.

---

### 2.2 Sampling retries vs Python SamplingClient

Python `SamplingClient`:

* Uses `RetryHandler` (`lib/retry_handler.py`) for sampling, with `max_retries`, jittered backoff, connection limits, *and* HTTP status–aware retry rules (5xx, 408, 429).
* Underneath that, `AsyncTinker`’s base client also has its own retries.

Your Elixir plan explicitly says:

> SamplingClient has RateLimiter for backoff, but does NOT retry. HTTP layer (TrainingClient/futures) handles retries.

And in `Tinkex.API.Sampling.asample/3` you set `max_retries: 0`, so it bypasses `with_retries/3`.

**Impact:**

* Compared to Python, sampling in Elixir will:

  * Not automatically retry transient 5xx/408 errors.
  * Rely entirely on user-level retry around `Task.await/1`, plus RateLimiter backoff on 429.
* That’s a *behavioral* divergence; in high-load scenarios Python will be more resilient out of the box.

**Recommendation:**

* Decide explicitly which behavior you want:

  * If you want parity with Python:
    let SamplingClient use the same retry pipeline as Training/futures (or its own `Tinkex.RetryHandler` analogue) and remove `max_retries: 0`.

  * If you *intentionally* want no automatic retries at the sampling layer:
    document this prominently as a divergence from Python instead of claiming parity.

---

### 2.3 Multi-base-URL vs Finch pools

You’ve documented the limitation correctly in prose, but the combination of `Tinkex.Config` and the Finch setup is a bit misleading:

* `Tinkex.Config.new/1` suggests you can freely set `base_url` per client.
* `Tinkex.Application.start/2` builds Finch pools once using a single `base_url` from `Application.get_env/3` and keys them by `{normalized_base, pool_type}`.
* `Tinkex.API.post/4` uses `pool: Tinkex.PoolKey.build(config.base_url, pool_type)`.

Result: any `config.base_url` whose normalized value differs from the app’s boot-time `base_url` will have no corresponding Finch pool and will crash.

You *do* note this limitation later (“single base_url per application instance”), but earlier sections read as if multi-base-URL multi-tenancy is supported.

**Suggestion:**

* Make this limitation very explicit near the `Tinkex.Config` definition and in the ServiceClient docs:

  > “`base_url` must match the application-wide base URL configured at startup; multiple base URLs in a single BEAM instance are not supported in v1.0.”

* Or implement dynamic pools (your Option 2) and drop the limitation.

---

## 3. Async / GenServer semantics

### 3.1 TrainingClient blocking behavior vs “queue” wording

You’ve done the right thing re: request ordering:

* `handle_call` for `{:forward_backward, ...}`:

  * Chunks data.
  * Sends all chunk requests *synchronously* in the GenServer process.
  * Then spawns a `Task.start` that polls futures and eventually calls `GenServer.reply(from, reply)`.
  * Returns `{:noreply, new_state}`.

You also explicitly document that this blocks the TrainingClient GenServer during the send phase. Good.

The only discrepancy is wording in places like `03_async_model.md` and `07_porting_strategy.md` that imply you have a proper work queue / `handle_continue` pattern implemented for responsiveness – but the shown code is the blocking version.

**Impact:**
Not a correctness bug, just doc drift: someone reading “optional work queue pattern for v2.0” might think it’s already there.

**Fix:**

* Make it explicit in the docs that v1.0 uses the simpler *blocking-in-handle_call* design and the work-queue + `handle_continue` pattern is a future enhancement / alternative sketch, *not* implemented.

---

### 3.2 ServiceClient API vs “all methods return Task”

In several places (e.g. in some “API consistency” bullets) you say:

> All public client methods return `Task.t({:ok, ...} | {:error, ...})`.

But your own examples show:

* `ServiceClient.create_lora_training_client/2` returning `{:ok, pid} | {:error, reason}` via `GenServer.call`.
* Only training / sampling operations (`forward_backward/4`, `optim_step/2`, `sample/4`) are Task-based.

Not a big deal, but it’s inconsistent phrasing.

**Suggestion:**
Clarify that *operation* methods (those that correspond to Python `APIFuture`s) return Tasks, while structural factory methods (create clients, etc.) return plain `{:ok, ...} | {:error, ...}`.

---

## 4. ETS & registry examples – stale snippets

You’ve evolved the SamplingClient design a few times:

* Early in `02_client_architecture.md`: `SamplingClient.init/1` writes directly into `:tinkex_sampling_clients` via `:ets.insert/2`, and `terminate/2` deletes its entry.
* Later (Round 5) you introduce `Tinkex.SamplingRegistry` that:

  * Monitors client processes.
  * Inserts into ETS on `register/2`.
  * Cleans up entries on `:DOWN`.

But both patterns still appear in the docs.

**Impact:**
If someone copy-pastes the “older” init snippet, you get two competing patterns: direct ETS insert in `init/1` *and* registry insert, or misaligned cleanup.

**Fix:**

* Remove / mark the direct `:ets.insert(:tinkex_sampling_clients, {...})` example as “pre-Registry design” and keep only the registry-based one.

---

## 5. Misc smaller nits

A few smaller things worth aligning:

1. **QueueState wire shape vs parsing:**

   * Python’s `QueueState` enum in `lib/api_future_impl.py` and `types/try_again_response.py` uses `"paused_rate_limit"` / `"paused_capacity"` etc.
   * Your Elixir `QueueState` type and `notify_queue_state_change/1` assume string values, which is fine, but make sure you parse from `queue_state` string consistently (you sometimes show atoms directly).

2. **First-metrics-only behavior:**

   * Python’s `_metrics_reduction` uses `keys = results[0].metrics.keys()`. If the backend *ever* returns a metric in later chunks only, Python ignores it; your union-based reducer will include it with zeros for earlier chunks. It’s probably harmless, but if you truly want 1:1 behavior, mirror the “first result defines the key set” behavior.

3. **Docstring vs reality on `prompt_logprobs` default:**

   * The Python docstring says “Defaults to false” while the default is actually `None`. You corrected behavior, but it might be worth explicitly noting “upstream docstring is misleading; we follow the actual default”.

4. **Sampling backoff responsibility:**

   * You now have both an HTTP-level retry mechanism and SamplingClient-level RateLimiter. Make sure the final docs clearly say:

     * “SamplingClient: backoff on 429, no automatic retry in v1.0.”
     * “Training & futures: automatic retries according to HTTP layer.”
     * Right now the split is described correctly in some sections, but earlier bullets still talk as if SamplingClient uses RetryHandler-like semantics.

---

## 6. Things that look solid / aligned

Just to sanity-check the positive side:

* AdamParams defaults (`learning_rate=1e-4`, `beta2=0.95`, `eps=1e-12`) match `types/optim_step_request.py`.
* `TensorDtype = Literal["int64","float32"]` and the aggressive Nx casting (f64→f32, s32→s64, u*→s64) faithfully reflect `tensor_dtype.py` + `_convert_*` helpers.
* Image types (`ImageChunk`, `ImageAssetPointerChunk`) match `image_chunk.py` and `image_asset_pointer_chunk.py` (`data` / `format` / `height` / `width` / `tokens` / `location` / `type`).
* StopReason (`"length" | "stop"`) matches `types/stop_reason.py`.
* `SampleRequest` fields and Optional semantics line up with `sample_request.py`.
* QueueState and TryAgainResponse exist exactly as you describe in the Python types.

So most of the foundation is correct—the main danger zones are:

* small stale snippets (especially around `prompt_logprobs` and SamplingClient ETS)
* “matches Python exactly” claims where you actually diverge (metrics reduction, Retry-After parsing, sampling retries)
* field-name mismatches for `TryAgainResponse` (`type` vs `status`).

If you clean those up, the plan will be much tighter and less likely to drift away from the true Python behavior during implementation.
