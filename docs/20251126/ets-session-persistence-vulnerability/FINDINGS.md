# Tinkex SessionManager ETS Persistence: Vulnerability Analysis

**Date:** November 26, 2025
**Severity:** High
**Status:** Investigation Complete - Fixes Pending
**Author:** Claude Code Analysis

---

## Executive Summary

Investigation into test warnings (`unknown registry`, `Heartbeat has failed`) revealed that the SessionManager's ETS-based session persistence has several edge case vulnerabilities that could manifest in production, not just tests.

**Core Issue:** SessionManager stores full `Config.t()` (including `http_pool` atom) in ETS but **never validates** that the referenced Finch pool still exists or is compatible when:
1. Loading sessions from ETS on restart
2. Before sending heartbeat requests

**Impact:** Zombie sessions that heartbeat forever against dead/incompatible pools, causing silent failures and resource waste.

---

## Table of Contents

1. [Discovery Context](#1-discovery-context)
2. [Architecture Analysis](#2-architecture-analysis)
3. [Edge Case Vulnerabilities](#3-edge-case-vulnerabilities)
4. [Debugging Strategies](#4-debugging-strategies)
5. [Recommended Fixes](#5-recommended-fixes)
6. [Test Scenarios](#6-test-scenarios)
7. [References](#7-references)

---

## 1. Discovery Context

### Original Symptoms

During concurrent test runs, warnings appeared:

```
[warning] Heartbeat has failed for session "session-123"
unknown registry: :finch_xxx
```

### Root Cause in Tests

The global `:tinkex_sessions` ETS table persists session data across concurrent tests. When one test's Finch pool shuts down, another SessionManager instance loads sessions from ETS that reference the now-dead pool.

**Reproduction:**
```bash
# Shows warnings
mix test test/tinkex/session_manager_test.exs test/tinkex/async_client_test.exs

# No warnings (serial execution avoids overlap)
mix test test/tinkex/session_manager_test.exs test/tinkex/async_client_test.exs --trace
```

### The Broader Question

If this happens in tests due to process lifecycle mismatches, **could similar issues occur in production?**

**Answer: Yes.** Several production scenarios can create the same mismatch between stored sessions and live Finch pools.

---

## 2. Architecture Analysis

### 2.1 ETS Table Lifecycle

**Location:** `lib/tinkex/application.ex:62-68`

```elixir
create_table(:tinkex_sessions, [
  :set,
  :public,
  :named_table,
  read_concurrency: true,
  write_concurrency: true
])
```

| Property | Value |
|----------|-------|
| **Created by** | `Tinkex.Application.start/2` |
| **Table name** | `:tinkex_sessions` (hardcoded) |
| **Type** | `:set` (unique keys) |
| **Access** | `:public` (any process can read/write) |
| **Ownership** | Application process |
| **Persistence** | In-memory only (lost on node restart) |
| **Heir** | None configured |

**Key Finding:** Table creation uses `try/rescue` for `ArgumentError`, allowing idempotent startup but masking potential issues.

### 2.2 Session Data Structure

**Location:** `lib/tinkex/session_manager.ex:14-18`

```elixir
@type session_entry :: %{
  config: Config.t(),           # Full config including http_pool atom
  last_success_ms: non_neg_integer(),
  last_error: term() | nil
}
```

**What's stored in `Config.t()`:**
- `base_url` - Stored at session creation time
- `api_key` - Stored at session creation time
- `http_pool` - Atom reference (e.g., `:tinkex_http_pool`)
- All other config fields

**Critical Issue:** The `http_pool` is stored as an **atom**, not validated at runtime. If the referenced process dies or changes, stored sessions become stale.

### 2.3 Session Loading on Startup

**Location:** `lib/tinkex/session_manager.ex:215-229`

```elixir
defp load_sessions_from_ets do
  try do
    if :ets.whereis(:tinkex_sessions) == :undefined do
      %{}
    else
      :ets.foldl(
        fn {session_id, entry}, acc -> Map.put(acc, session_id, entry) end,
        %{},
        :tinkex_sessions
      )
    end
  rescue
    ArgumentError -> %{}
  end
end
```

**No validation occurs:**
- No check that `http_pool` atom refers to a running process
- No check that `base_url` is still reachable
- No check that session is still valid on remote server
- No check that config matches current application config

### 2.4 Supervision Tree

**Location:** `lib/tinkex/application.ex:32-36`

```elixir
Supervisor.start_link(children, strategy: :one_for_one, name: Tinkex.Supervisor)
```

**Child Order:**
1. Finch HTTP Pool (conditional)
2. `Tinkex.Metrics`
3. `Tinkex.RetrySemaphore`
4. `Tinkex.SamplingRegistry`
5. `Task.Supervisor`
6. **`Tinkex.SessionManager`**
7. `DynamicSupervisor` (ClientSupervisor)

**Strategy:** `:one_for_one`

**Implications:**
- If Finch crashes → Only Finch restarts (SessionManager keeps stale references)
- If SessionManager crashes → Only SessionManager restarts (reloads potentially stale sessions from ETS)
- No dependency management between Finch and SessionManager

### 2.5 Heartbeat Error Handling

**Location:** `lib/tinkex/session_manager.ex:162-177`

```elixir
defp send_heartbeat(session_id, %Config{} = config, session_api) do
  case safe_heartbeat(session_id, config, session_api) do
    {:ok, _} -> :ok
    {:error, %Error{} = error} ->
      Logger.debug("Heartbeat failed for #{session_id}: #{Error.format(error)}")
      {:error, error}
    {:error, reason} ->
      Logger.debug("Heartbeat failed for #{session_id}: #{inspect(reason)}")
      {:error, reason}
  end
end
```

**Current Behavior on Failure:**
1. Error logged at **debug level** (not visible in production)
2. Session **remains in tracking** (line 114)
3. Session **persists in ETS** with `last_error` set
4. Warning logged only after `heartbeat_warning_after_ms` (default 120s)
5. **Retries forever** - no circuit breaker, no removal

---

## 3. Edge Case Vulnerabilities

### 3.1 Edge Case A: Hot Code Reload / Config Change

**Scenario:** Application config changes `http_pool` from `:my_finch` to `:my_finch_v2`. Old sessions still reference `:my_finch`.

| Property | Value |
|----------|-------|
| **Possible?** | YES |
| **Severity** | Medium |
| **Detection** | Warning after 120s (if pool is dead) or silent misdirection (if pool exists with different config) |

**Sequence:**
1. Session created with `Config{http_pool: :my_finch, base_url: "https://old.api.com"}`
2. Config changed: `http_pool: :my_finch_v2, base_url: "https://new.api.com"`
3. Old pool `:my_finch` either:
   - Doesn't exist → `unknown registry` error
   - Still exists → Requests go to **wrong destination**
4. Session stays in tracking, heartbeat retries forever

**Real-World Trigger:** Blue-green deployments, canary releases, config management changes.

### 3.2 Edge Case B: Finch Crash Without SessionManager Restart

**Scenario:** Finch pool crashes, supervisor restarts it with new PID (same atom name). SessionManager continues with existing session references.

| Property | Value |
|----------|-------|
| **Possible?** | YES |
| **Severity** | Medium |
| **Detection** | Transient errors during pool initialization, then recovery |

**Sequence:**
1. Finch at `:tinkex_http_pool` crashes
2. Supervisor restarts Finch with new PID, same atom
3. SessionManager's sessions still reference `:tinkex_http_pool`
4. Next heartbeat:
   - Registry lookup succeeds (same atom)
   - Request may fail if pool not fully initialized
   - Or succeed if pool is ready

**Complication:** If Finch restarts with **different config** (different base URL, TLS settings), requests succeed but go to wrong destination.

### 3.3 Edge Case C: SessionManager Crash / Restart

**Scenario:** SessionManager crashes, supervisor restarts it, it loads all sessions from ETS without validation.

| Property | Value |
|----------|-------|
| **Possible?** | YES |
| **Severity** | **High** |
| **Detection** | Silent - sessions resume with potentially invalid config |

**Sequence:**
1. SessionManager crashes (OOM, bug, external signal)
2. Supervisor restarts SessionManager
3. `init/1` calls `load_sessions_from_ets()`
4. **All sessions loaded blindly** - including those referencing:
   - Dead pools
   - Reconfigured pools
   - Pools that never existed (test artifacts)
5. Heartbeats fail silently, sessions become zombies

**This is the highest-risk scenario** because it can happen from any SessionManager crash.

### 3.4 Edge Case D: Partial Application Restart (One-For-One Chaos)

**Scenario:** Multiple children crash at different times due to `:one_for_one` strategy, creating inconsistent state.

| Property | Value |
|----------|-------|
| **Possible?** | YES |
| **Severity** | **High** |
| **Detection** | Subtle - requires state inspection to detect mismatch |

**Example Sequence:**
```
T0: Finch crashes, restarts with config v2 (base_url changed)
T1: Client creates new session (uses config v2)
T2: SessionManager crashes, restarts
T3: SessionManager loads OLD sessions from ETS (config v1)
T4: Old sessions heartbeat against v2 pool with v1 base_url
```

**Result:** Requests constructed with old base_url sent through new pool - undefined behavior.

### 3.5 Edge Case E: BEAM Node Restart with Persistent ETS

**Scenario:** If DETS or external persistence is added for crash recovery, old sessions could survive node restarts.

| Property | Value |
|----------|-------|
| **Possible?** | NOT CURRENTLY |
| **Severity** | **Critical** (if persistence added) |
| **Detection** | N/A currently |

**Current Safeguard:** Pure in-memory ETS is lost on node restart, accidentally preventing this issue.

**Future Risk:** If someone adds DETS-backed persistence for "crash recovery," stale sessions would persist indefinitely.

---

## 4. Debugging Strategies

### 4.1 Runtime Diagnostics (IEx / Remote Console)

```elixir
# Check if pool exists
Process.whereis(:tinkex_http_pool)
# => #PID<0.123.0> or nil

# Get pool status
:sys.get_status(:tinkex_http_pool)

# Check ETS table exists
:ets.whereis(:tinkex_sessions)
# => reference or :undefined

# Count sessions in ETS
:ets.info(:tinkex_sessions, :size)

# List all sessions
:ets.tab2list(:tinkex_sessions)

# Find zombie sessions (failing for >120s)
:ets.tab2list(:tinkex_sessions)
|> Enum.filter(fn {_id, entry} ->
  now = System.monotonic_time(:millisecond)
  entry.last_error != nil and (now - entry.last_success_ms) > 120_000
end)

# Check SessionManager state
:sys.get_state(Tinkex.SessionManager)

# Group sessions by error status
sessions = :sys.get_state(Tinkex.SessionManager).sessions
Enum.group_by(sessions, fn {_id, entry} -> entry.last_error != nil end)
# => %{false => [healthy...], true => [failing...]}
```

### 4.2 Recommended Logging Additions

```elixir
# Session lifecycle (info level)
Logger.info("SessionManager: created session=#{session_id} pool=#{config.http_pool}")
Logger.info("SessionManager: stopped session=#{session_id}")

# Pool validation on load (warning level)
Logger.warning("SessionManager: loaded session=#{session_id} with dead pool=#{pool}")

# First heartbeat failure (warning level, not just after 120s)
Logger.warning("SessionManager: heartbeat_error session=#{session_id} error=#{error}")
```

### 4.3 Telemetry Events to Add

```elixir
# Emit on session lifecycle
:telemetry.execute(
  [:tinkex, :session, :start],
  %{count: 1},
  %{session_id: session_id, http_pool: config.http_pool}
)

:telemetry.execute(
  [:tinkex, :session, :stop],
  %{count: 1},
  %{session_id: session_id, reason: reason}
)

# Emit on heartbeat
:telemetry.execute(
  [:tinkex, :heartbeat, :success],
  %{duration_ms: duration},
  %{session_id: session_id}
)

:telemetry.execute(
  [:tinkex, :heartbeat, :failure],
  %{count: 1},
  %{session_id: session_id, error: error, pool: config.http_pool}
)
```

### 4.4 Observer / Recon Commands

```elixir
# With recon library
:recon.get_state(Tinkex.SessionManager)
:recon.proc_count(:memory, 10)  # Find memory-heavy processes
:recon.info(pid, :messages)     # Check mailbox

# With observer
:observer.start()
# Navigate to: Applications -> tinkex -> SessionManager
```

---

## 5. Recommended Fixes

### 5.1 Priority 1: Validate Sessions on Load (CRITICAL)

**Problem:** `load_sessions_from_ets/0` loads all sessions without validation.

**Fix:** Add validation, skip/remove invalid sessions.

```elixir
defp load_sessions_from_ets do
  try do
    if :ets.whereis(:tinkex_sessions) == :undefined do
      %{}
    else
      :ets.foldl(
        fn {session_id, entry}, acc ->
          case validate_session_on_load(session_id, entry) do
            :ok ->
              Map.put(acc, session_id, entry)
            {:error, reason} ->
              Logger.warning(
                "SessionManager: discarding invalid session=#{session_id} reason=#{reason}"
              )
              :ets.delete(:tinkex_sessions, session_id)
              acc
          end
        end,
        %{},
        :tinkex_sessions
      )
    end
  rescue
    ArgumentError -> %{}
  end
end

defp validate_session_on_load(session_id, %{config: %Config{http_pool: pool}}) do
  case Process.whereis(pool) do
    nil ->
      {:error, "http_pool #{inspect(pool)} not registered"}
    _pid ->
      :ok
  end
end
```

### 5.2 Priority 2: Pool Health Check Before Heartbeat (HIGH)

**Problem:** Heartbeat sends requests without verifying pool exists.

**Fix:** Pre-flight check before each heartbeat.

```elixir
defp send_heartbeat(session_id, %Config{http_pool: pool} = config, session_api) do
  case Process.whereis(pool) do
    nil ->
      Logger.warning(
        "SessionManager: skipping heartbeat session=#{session_id} pool=#{pool} not alive"
      )
      {:error, :pool_not_alive}

    _pid ->
      do_send_heartbeat(session_id, config, session_api)
  end
end
```

### 5.3 Priority 3: Session Invalidation After Repeated Failures (MEDIUM)

**Problem:** Sessions with persistent failures remain forever.

**Fix:** Remove sessions after threshold exceeded.

```elixir
@max_failure_duration_ms 600_000  # 10 minutes
@max_consecutive_failures 10

defp handle_heartbeat_result(session_id, entry, :ok, state) do
  updated = %{entry | last_success_ms: now_ms(), last_error: nil, failure_count: 0}
  persist_session(session_id, updated)
  {:ok, updated}
end

defp handle_heartbeat_result(session_id, entry, {:error, error}, state) do
  failure_count = Map.get(entry, :failure_count, 0) + 1
  time_failing = now_ms() - entry.last_success_ms

  cond do
    failure_count >= @max_consecutive_failures ->
      Logger.warning("SessionManager: removing session=#{session_id} after #{failure_count} failures")
      remove_session(session_id)
      {:removed, :max_failures}

    time_failing >= @max_failure_duration_ms ->
      Logger.warning("SessionManager: removing session=#{session_id} after #{time_failing}ms failing")
      remove_session(session_id)
      {:removed, :timeout}

    true ->
      updated = %{entry | last_error: error, failure_count: failure_count}
      persist_session(session_id, updated)
      {:error, updated}
  end
end
```

### 5.4 Priority 4: Supervision Strategy Change (MEDIUM)

**Problem:** `:one_for_one` doesn't manage Finch/SessionManager dependency.

**Fix:** Use `:rest_for_one` to restart SessionManager when Finch restarts.

```elixir
# In Tinkex.Application.start/2
Supervisor.start_link(children, strategy: :rest_for_one, name: Tinkex.Supervisor)
```

**Trade-off:** Finch restart now causes SessionManager restart (brief heartbeat pause), but ensures consistency.

**Alternative:** Keep `:one_for_one` but add process monitoring:

```elixir
# In SessionManager.init/1
if pool = Application.get_env(:tinkex, :http_pool) do
  case Process.whereis(pool) do
    nil -> :ok
    pid -> Process.monitor(pid)
  end
end

# Handle pool death
def handle_info({:DOWN, _ref, :process, _pid, _reason}, state) do
  Logger.warning("SessionManager: http_pool died, clearing sessions with that pool")
  # Invalidate affected sessions
  {:noreply, invalidate_sessions_for_pool(state)}
end
```

### 5.5 Priority 5: Configurable ETS Table Name (LOW)

**Problem:** Hardcoded `:tinkex_sessions` prevents test isolation.

**Fix:** Make table name configurable.

```elixir
# In config
config :tinkex, :sessions_table, :tinkex_sessions

# In SessionManager
@sessions_table Application.compile_env(:tinkex, :sessions_table, :tinkex_sessions)

# In tests
Application.put_env(:tinkex, :sessions_table, :"tinkex_sessions_#{System.unique_integer()}")
```

---

## 6. Test Scenarios

### 6.1 Test: Pool Validation on Load

```elixir
test "discards sessions with dead pool on load" do
  # Setup: Create session, kill pool, restart SessionManager
  {:ok, session} = SessionManager.start_session(config)

  # Kill the pool
  GenServer.stop(Process.whereis(:test_finch_pool))

  # Restart SessionManager
  Supervisor.terminate_child(Tinkex.Supervisor, Tinkex.SessionManager)
  Supervisor.restart_child(Tinkex.Supervisor, Tinkex.SessionManager)

  # Session should NOT be loaded
  refute Map.has_key?(get_sessions(), session)

  # ETS should be cleaned
  assert :ets.lookup(:tinkex_sessions, session) == []
end
```

### 6.2 Test: Heartbeat Skips Dead Pool

```elixir
test "heartbeat skips session with dead pool" do
  {:ok, session} = SessionManager.start_session(config)

  # Kill the pool (don't restart)
  GenServer.stop(Process.whereis(:test_finch_pool))

  # Trigger heartbeat
  send(SessionManager, :heartbeat)

  # Should log warning, not crash
  assert_receive {:log, :warning, msg}
  assert msg =~ "pool not alive"
end
```

### 6.3 Test: Session Removed After Max Failures

```elixir
test "removes session after max consecutive failures" do
  {:ok, session} = SessionManager.start_session(config)

  # Make heartbeat fail repeatedly
  Bypass.down(bypass)

  for _ <- 1..10 do
    send(SessionManager, :heartbeat)
    Process.sleep(10)
  end

  # Session should be removed
  refute Map.has_key?(get_sessions(), session)
end
```

---

## 7. References

### Source Files Analyzed

| File | Key Lines | Purpose |
|------|-----------|---------|
| `lib/tinkex/application.ex` | 32-68 | Supervision tree, ETS creation |
| `lib/tinkex/session_manager.ex` | 14-18, 62-75, 162-177, 215-229 | Session storage, loading, heartbeat |
| `lib/tinkex/config.ex` | 35, 80 | http_pool field definition |
| `lib/tinkex/api.ex` | 487 | Finch.request call |

### Related Issues

- Test warnings: `unknown registry: :finch_xxx`
- Test warnings: `Heartbeat has failed for session "session-123"`

### Design Decisions to Document

1. **Sessions are anchored to config at creation time** - by design, to prevent base_url changes mid-session
2. **Heartbeat retries forever** - matches Python client behavior (per code comment)
3. **ETS is in-memory only** - no crash recovery, but also no stale persistence

---

## Appendix: Summary Table

| Edge Case | Possible | Severity | Current Behavior | Recommended Fix |
|-----------|----------|----------|------------------|-----------------|
| Hot config reload | YES | Medium | Silent failure/misdirection | Pool validation |
| Finch crash | YES | Medium | Transient errors | Pool monitoring |
| SessionManager crash | YES | **High** | Loads stale sessions | Validate on load |
| Partial restart | YES | **High** | Inconsistent state | `:rest_for_one` or monitoring |
| Persistent ETS | Not yet | Critical | N/A (in-memory) | Add TTL if persistence added |
