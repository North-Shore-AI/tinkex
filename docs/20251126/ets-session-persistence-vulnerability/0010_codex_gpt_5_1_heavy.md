# ETS Session Persistence â€” Production Edge Cases (Superseding Note)

**Author:** Codex (GPT-5.1 Heavy)  \
**Date:** 2025-11-26  \
**Status:** Supersedes prior test-focused writeup; scoping to realistic production risks

## Why this note

The earlier "vulnerability" doc conflated a test-only flake (shared ETS between concurrently running test managers) with production behavior. In the shipped topology (one SessionManager, one supervised Finch pool) the "unknown registry" warnings do not manifest. This note captures the actual edge cases that can occur in production and suggested mitigations.

## Current architecture (facts)

- Session state lives in a global, hardcoded ETS table `:tinkex_sessions` (public, set, read/write concurrency). It stores full `Config` structs, including secrets.
- SessionManager reloads the ETS table on restart without validation; it keeps heartbeating indefinitely on all failures and only warns after `heartbeat_warning_after_ms` (default 120s).
- Finch is started once under the application supervisor; the Finch spec is static and does not derive from `base_url` at runtime. `Tinkex.API` uses the configured pool atom, not the computed pool key, when calling Finch.

## Production-relevant edge cases

1) **Multiple SessionManagers in the same node (non-default integration)**  
   - If an integrator starts more than one SessionManager/Finch pair, they all share `:tinkex_sessions`. A manager restart will load sessions created by another manager and heartbeat them on its own pool. Default deploys do not do this, but the cross-talk is real if someone embeds multiple managers.

2) **SessionManager crash/restart + pool change or missing pool**  
   - On restart we reload ETS blindly. If the configured `http_pool` was changed, or a user-managed Finch pool is down, we keep heartbeating using the stale config and log only after 120s. Sessions are never dropped. This can generate sustained warnings and wasted retries.

3) **Externally managed Finch dies permanently**  
   - If the host app supplies its own Finch under a custom atom and that process exits, SessionManager will continue heartbeating forever against a dead pool atom. Recovery happens only if the host restarts Finch under the same name.

4) **Config drift across deploys with surviving ETS**  
   - ETS is in-memory, so it does not survive node restarts, but it does survive a SessionManager crash. If app config (base_url/http_pool) changes and SessionManager restarts without clearing ETS, old sessions continue using the old config. Requests can go to the wrong destination or fail noisily until cleaned.

5) **Secret exposure in ETS**  
   - `:tinkex_sessions` is `:public` and stores API keys and Cloudflare Access secrets. Any process on the node can read them. Not a stability bug, but a security footgun.

## What is *not* a production risk

- The `unknown registry` / "zombie heartbeat" issue in tests is caused by concurrent test managers reusing the global ETS table. Default runtime (single manager + single pool) does not hit this.

## Mitigation options (ordered by safety/impact)

1) **Table protection and naming (safe)**  
   - Change `:tinkex_sessions` to `:protected` (read/write by owner, read by others) to reduce secret exposure.  
   - Add a configurable `:sessions_table` name to allow multi-manager apps/tests to isolate state. Low-risk, backward-compatible with a default of `:tinkex_sessions`.

2) **Visibility instead of drop (moderate)**  
   - On SessionManager init, validate each entry: if `Process.whereis(config.http_pool)` is nil, log a warning and skip scheduling heartbeats for that entry until the pool appears (do not delete). This surfaces misconfig/missing pools without breaking parity on persistence.

3) **Optional cleanup knobs (opt-in, parity-aware)**  
   - Provide opt-in config for max failure duration/count before removing a session. Default keeps current "never drop" behavior to stay aligned with Python, but allows operators to enable cleanup in hosts that prefer it.

4) **Supervision coupling (optional)**  
   - If we want SessionManager to restart when Finch restarts, switch to `:rest_for_one` or monitor the pool pid and clear/resume sessions accordingly. This changes failure domains; should be a deliberate choice.

## Recommendations

- Implement (1) to address the real cross-talk and secret exposure with minimal behavior change.  
- Consider (2) for operational visibility; it avoids silent retries against nonexistent pools without deleting sessions.  
- Leave cleanup/removal (3) off by default unless product wants divergence from current retry-forever semantics.  
- If a host app runs multiple managers, document the `:sessions_table` override to isolate their state.
