Agent Prompt: Gap #6 - Streaming API Parity

Task Overview

You are implementing true streaming support for Tinkex, an Elixir port of the Tinker Python SDK. The Python SDK provides O(1) memory streaming for both SSE events and binary downloads. The Elixir
implementation currently downloads entire responses to RAM before processing ("fake streaming"), causing O(n) memory usage that will fail for large files, and its streaming path bypasses the shared telemetry/retry/dump-headers pipeline (no backoff, no observability, no pool selection).

Required Reading

Read these files in order before implementation:

Gap Analysis Document:
- ./docs/20251127/gaps_06/06_streaming_api_parity.md

Python Reference Implementation:
- ./tinker/src/tinker/_streaming.py - Stream/AsyncStream classes with lazy iteration, SSEDecoder, SSEBytesDecoder
- ./tinker/src/tinker/_response.py - APIResponse, BinaryAPIResponse, StreamedBinaryAPIResponse classes

Elixir Current Implementation:
- ./lib/tinkex/streaming/sse_decoder.ex - SSEDecoder designed for incremental use but called eagerly
- ./lib/tinkex/checkpoint_download.ex - Uses :httpc with body_format: :binary (full download)
- ./lib/tinkex/api/api.ex - Current API request handling; note `stream_get/2` bypasses `execute_with_telemetry/3`, `with_retries/6`, dump headers, and pool selection
- ./lib/tinkex/api/helpers.ex - `with_streaming_response/1` sets `response: :stream` but `get/post/delete` ignore it
- ./lib/tinkex/future.ex - Future polling mechanism

Supporting Context:
- ./lib/tinkex/sampling_client.ex - Example of client using streaming SSE
- ./lib/tinkex/types/model_input.ex - Type patterns in the codebase

Gap Explanation

Python Streaming Architecture:

1. SSE Streaming - Stream[T] class wraps response iterator:
class Stream(Generic[_T]):
    def __init__(self, *, cast_to: type[_T], response: httpx.Response, client: ...):
        self.response = response
        self._decoder = SSEBytesDecoder()  # Incremental decoder
        self._iterator = self.__stream__()

    def __iter__(self) -> Iterator[_T]:
        for item in self._iterator:
            yield item

    def __stream__(self) -> Iterator[_T]:
        for sse in self._decoder.iter_bytes(self.response.iter_bytes()):
            # Process one SSE event at a time - O(1) memory
            yield process_sse(sse)
2. Binary Streaming - StreamedBinaryAPIResponse:
class StreamedBinaryAPIResponse(APIResponse[bytes]):
    def iter_bytes(self, chunk_size: int | None = None) -> Iterator[bytes]:
        for chunk in self.response.iter_bytes(chunk_size):
            yield chunk  # Never holds more than chunk_size in memory

    def stream_to_file(self, path: str | Path, chunk_size: int | None = None):
        with open(path, "wb") as f:
            for chunk in self.iter_bytes(chunk_size):
                f.write(chunk)  # Stream directly to disk
3. SSEDecoder - Incremental byte-level parsing:
class SSEBytesDecoder:
    def __init__(self):
        self._data = b""

    def iter_bytes(self, iterator: Iterator[bytes]) -> Iterator[ServerSentEvent]:
        for chunk in iterator:
            self._data += chunk
            # Yield complete events, keep partial in buffer
            for event in self._extract_events():
                yield event

Elixir Current Problems:

1. Checkpoint Download - Loads entire file into RAM:
# checkpoint_download.ex - PROBLEM: full file in memory
def download(url, path, _opts) do
case :httpc.request(:get, {url, headers}, http_opts, body_format: :binary) do
    {:ok, {{_, 200, _}, _, body}} ->
    File.write!(path, body)  # body is entire file in RAM!
end
end
2. SSE Decoder - Designed for incremental but used eagerly:
# The decoder CAN handle incremental chunks...
def feed(%__MODULE__{} = decoder, chunk) do
{events, rest} = parse_events(decoder.buffer <> chunk, [])
{Enum.reverse(events), %__MODULE__{buffer: rest}}
end

# ...but callers download everything first, then parse
3. No Stream Abstraction - No equivalent to Python's Stream[T] wrapper
4. Streaming path bypasses resilience/observability - `stream_get/2` uses `Finch.request/3` directly, skipping `with_retries/6`, `execute_with_telemetry/3`, pool selection, retry headers, and request dumps; `response: :stream` is ignored by `get/post/delete`

Implementation Requirements

Phase 1: Core Streaming Infrastructure

1.1 Create Tinkex.Stream module (./lib/tinkex/stream.ex):
defmodule Tinkex.Stream do
@moduledoc """
Lazy stream wrapper for API responses. Provides O(1) memory iteration
over SSE events or binary chunks.
"""

defstruct [:response_stream, :decoder, :cast_to, :client_ref]

@type t(item) :: %__MODULE__{
    response_stream: Enumerable.t(),
    decoder: module(),
    cast_to: module(),
    client_ref: reference()
}

@doc "Create stream from Finch response"
@spec from_response(Finch.Response.t(), keyword()) :: t(term())

@doc "Iterate over decoded items - implements Enumerable"
# Implement Enumerable protocol for lazy iteration
end

1.2 Create Tinkex.Streaming.BinaryStream (./lib/tinkex/streaming/binary_stream.ex):
defmodule Tinkex.Streaming.BinaryStream do
@moduledoc """
Binary response streaming with O(1) memory. Supports chunked iteration
and streaming directly to files.
"""

@type t :: %__MODULE__{
    response_stream: Enumerable.t(),
    chunk_size: pos_integer()
}

@doc "Iterate over binary chunks"
@spec iter_bytes(t(), keyword()) :: Enumerable.t()

@doc "Stream response directly to file without loading into memory"
@spec stream_to_file(t(), Path.t(), keyword()) :: :ok | {:error, term()}
end

1.3 Update Tinkex.Streaming.SSEDecoder - Add byte-level incremental API:
defmodule Tinkex.Streaming.SSEBytesDecoder do
@moduledoc """
Byte-level SSE decoder for true streaming. Wraps SSEDecoder with
binary chunk handling.
"""

@doc "Create iterator over SSE events from byte stream"
@spec iter_bytes(Enumerable.t()) :: Enumerable.t()
end

Phase 2: Finch Streaming Integration

2.1 Create streaming request function in API layer:
defmodule Tinkex.API.Streaming do
@moduledoc """
Streaming HTTP requests using Finch's streaming capabilities.
"""

@doc """
Make streaming GET request. Returns lazy enumerable of chunks.
Uses Finch.stream/5 for true streaming without buffering.
"""
@spec get_stream(String.t(), keyword()) :: {:ok, Enumerable.t()} | {:error, term()}

@doc """
Make streaming POST request for SSE responses.
"""
@spec post_stream(String.t(), term(), keyword()) :: {:ok, Enumerable.t()} | {:error, term()}
end

2.2 Finch streaming pattern:
def get_stream(url, opts) do
# Use Finch.stream which yields chunks as they arrive
stream = Stream.resource(
    fn -> start_request(url, opts) end,
    fn state -> receive_chunk(state) end,
    fn state -> cleanup(state) end
)
{:ok, stream}
end

2.3 Preserve telemetry/retries/logging parity:
- Streaming requests must reuse `execute_with_telemetry/3` + `with_retries/6` (or equivalent) so telemetry events, retry headers, dump-headers behavior, and pool selection (`PoolKey`) match non-streaming calls.

Phase 3: Update Checkpoint Download

3.1 Rewrite Tinkex.CheckpointDownload to use true streaming:
defmodule Tinkex.CheckpointDownload do
alias Tinkex.Streaming.BinaryStream

@doc """
Download checkpoint with O(1) memory usage.
Streams directly to file without buffering entire response.
"""
def download(url, path, opts \\ []) do
    chunk_size = Keyword.get(opts, :chunk_size, 64 * 1024)  # 64KB chunks

    with {:ok, stream} <- BinaryStream.from_url(url, opts) do
    BinaryStream.stream_to_file(stream, path, chunk_size: chunk_size)
    end
end

@doc """
Download with progress callback for large files.
"""
def download_with_progress(url, path, progress_callback, opts \\ [])
end

Phase 4: SSE Stream Integration

4.1 Update Future polling to use streaming SSE:
# In Tinkex.Future - use streaming for poll responses
defp poll_with_streaming(request_id, opts) do
with {:ok, sse_stream} <- API.Streaming.post_stream(poll_url, body, opts) do
    sse_stream
    |> SSEBytesDecoder.iter_bytes()
    |> Enum.reduce_while(nil, fn event, _acc ->
    case process_event(event) do
        {:done, result} -> {:halt, {:ok, result}}
        {:continue, _} -> {:cont, nil}
        {:error, e} -> {:halt, {:error, e}}
    end
    end)
end
end

TDD Implementation Plan

Test File: test/tinkex/stream_test.exs

defmodule Tinkex.StreamTest do
use ExUnit.Case, async: true

alias Tinkex.Stream

describe "from_response/2" do
    test "creates lazy stream from enumerable" do
    chunks = ["chunk1", "chunk2", "chunk3"]
    stream = Stream.from_response(chunks, cast_to: :raw)

    # Stream should be lazy - no processing yet
    assert %Stream{} = stream

    # Enumeration triggers processing
    result = Enum.to_list(stream)
    assert result == chunks
    end

    test "processes chunks incrementally without buffering" do
    # Track memory to verify O(1) behavior
    large_chunks = Stream.repeatedly(fn -> :crypto.strong_rand_bytes(1024) end)
                    |> Stream.take(1000)

    stream = Stream.from_response(large_chunks, cast_to: :raw)

    # Process should not spike memory
    count = Enum.reduce(stream, 0, fn _chunk, acc -> acc + 1 end)
    assert count == 1000
    end
end

describe "Enumerable protocol" do
    test "supports Enum.take for partial consumption" do
    chunks = 1..100 |> Enum.map(&"chunk#{&1}")
    stream = Stream.from_response(chunks, cast_to: :raw)

    # Should only process first 5
    result = Enum.take(stream, 5)
    assert length(result) == 5
    end

    test "supports Stream.map for lazy transformation" do
    chunks = ["a", "b", "c"]
    stream = Stream.from_response(chunks, cast_to: :raw)

    transformed = stream
                    |> Stream.map(&String.upcase/1)
                    |> Enum.to_list()

    assert transformed == ["A", "B", "C"]
    end
end
end

Test File: test/tinkex/streaming/binary_stream_test.exs

defmodule Tinkex.Streaming.BinaryStreamTest do
use ExUnit.Case, async: true

alias Tinkex.Streaming.BinaryStream

describe "iter_bytes/2" do
    test "yields chunks of specified size" do
    data = :crypto.strong_rand_bytes(1000)
    chunks = [data]
    stream = BinaryStream.new(chunks, chunk_size: 100)

    result = stream |> BinaryStream.iter_bytes() |> Enum.to_list()

    # Should have 10 chunks of 100 bytes each
    assert length(result) == 10
    assert Enum.all?(result, &(byte_size(&1) == 100))
    assert IO.iodata_to_binary(result) == data
    end

    test "handles partial final chunk" do
    data = :crypto.strong_rand_bytes(250)
    stream = BinaryStream.new([data], chunk_size: 100)

    result = stream |> BinaryStream.iter_bytes() |> Enum.to_list()

    assert length(result) == 3
    assert byte_size(List.last(result)) == 50
    end
end

describe "stream_to_file/3" do
    @tag :tmp_dir
    test "streams to file without loading into memory", %{tmp_dir: tmp_dir} do
    # Create 10MB of data in chunks
    chunk_size = 64 * 1024
    chunk_count = 160  # ~10MB total

    chunks = Stream.repeatedly(fn -> :crypto.strong_rand_bytes(chunk_size) end)
                |> Stream.take(chunk_count)

    stream = BinaryStream.new(chunks)
    path = Path.join(tmp_dir, "test_download.bin")

    # Memory should stay constant during streaming
    initial_memory = :erlang.memory(:binary)

    :ok = BinaryStream.stream_to_file(stream, path)

    final_memory = :erlang.memory(:binary)

    # File should exist with correct size
    assert File.exists?(path)
    assert File.stat!(path).size == chunk_size * chunk_count

    # Memory increase should be much less than file size
    memory_increase = final_memory - initial_memory
    assert memory_increase < chunk_size * 10  # Allow some overhead
    end

    test "creates parent directories if needed", %{tmp_dir: tmp_dir} do
    chunks = ["test data"]
    stream = BinaryStream.new(chunks)
    path = Path.join([tmp_dir, "nested", "dir", "file.bin"])

    :ok = BinaryStream.stream_to_file(stream, path, mkdir: true)

    assert File.exists?(path)
    end

    test "returns error on write failure" do
    chunks = ["test"]
    stream = BinaryStream.new(chunks)

    result = BinaryStream.stream_to_file(stream, "/nonexistent/path/file.bin")

    assert {:error, _reason} = result
    end
end

describe "progress tracking" do
    test "calls progress callback with bytes written" do
    chunks = ["abc", "defgh", "ij"]  # 10 bytes total
    stream = BinaryStream.new(chunks)

    {:ok, agent} = Agent.start_link(fn -> [] end)

    progress_fn = fn bytes_written, total_bytes ->
        Agent.update(agent, &[{bytes_written, total_bytes} | &1])
    end

    path = Path.join(System.tmp_dir!(), "progress_test.bin")
    :ok = BinaryStream.stream_to_file(stream, path,
        progress: progress_fn,
        total_size: 10
    )

    progress = Agent.get(agent, &Enum.reverse/1)
    assert progress == [{3, 10}, {8, 10}, {10, 10}]

    File.rm!(path)
    end
end
end

Test File: test/tinkex/streaming/sse_bytes_decoder_test.exs

defmodule Tinkex.Streaming.SSEBytesDecoderTest do
use ExUnit.Case, async: true

alias Tinkex.Streaming.{SSEBytesDecoder, ServerSentEvent}

describe "iter_bytes/1" do
    test "decodes complete events from byte stream" do
    chunks = [
        "event: message\n",
        "data: {\"hello\": \"world\"}\n\n"
    ]

    events = chunks
                |> SSEBytesDecoder.iter_bytes()
                |> Enum.to_list()

    assert [%ServerSentEvent{event: "message", data: data}] = events
    assert data == "{\"hello\": \"world\"}"
    end

    test "handles events split across chunks" do
    # Event split mid-field
    chunks = [
        "event: test\nda",
        "ta: part1\ndata: part2\n\n"
    ]

    events = chunks
                |> SSEBytesDecoder.iter_bytes()
                |> Enum.to_list()

    assert [%ServerSentEvent{data: "part1\npart2"}] = events
    end

    test "yields events as soon as complete" do
    chunks = [
        "data: first\n\ndata: second\n\n",
        "data: third\n\n"
    ]

    events = chunks
                |> SSEBytesDecoder.iter_bytes()
                |> Enum.to_list()

    assert length(events) == 3
    assert Enum.map(events, & &1.data) == ["first", "second", "third"]
    end

    test "handles empty data events" do
    chunks = ["data: \n\n"]

    events = chunks
                |> SSEBytesDecoder.iter_bytes()
                |> Enum.to_list()

    assert [%ServerSentEvent{data: ""}] = events
    end

    test "ignores comment lines" do
    chunks = [": this is a comment\ndata: actual\n\n"]

    events = chunks
                |> SSEBytesDecoder.iter_bytes()
                |> Enum.to_list()

    assert [%ServerSentEvent{data: "actual"}] = events
    end

    test "preserves partial event in buffer between chunks" do
    chunks = [
        "data: incomplete",  # No \n\n yet
        " more data\n\n"
    ]

    events = chunks
                |> SSEBytesDecoder.iter_bytes()
                |> Enum.to_list()

    # Should combine into single event
    assert [%ServerSentEvent{data: "incomplete more data"}] = events
    end
end

describe "memory efficiency" do
    test "does not buffer entire stream" do
    # Generate many events
    event_count = 10_000
    chunks = Stream.repeatedly(fn -> "data: test\n\n" end)
                |> Stream.take(event_count)

    # Process lazily and count
    count = chunks
            |> SSEBytesDecoder.iter_bytes()
            |> Enum.reduce(0, fn _event, acc -> acc + 1 end)

    assert count == event_count
    end
end
end

Test File: test/tinkex/checkpoint_download_streaming_test.exs

defmodule Tinkex.CheckpointDownloadStreamingTest do
use ExUnit.Case, async: true

alias Tinkex.CheckpointDownload

# Integration tests with mock HTTP server

describe "download/3 streaming" do
    @tag :tmp_dir
    test "downloads large file with constant memory", %{tmp_dir: tmp_dir} do
    # This test requires a mock server or bypass
    # Testing the streaming behavior, not actual HTTP

    path = Path.join(tmp_dir, "checkpoint.bin")

    # Mock streaming response
    mock_stream = Stream.repeatedly(fn -> :crypto.strong_rand_bytes(1024) end)
                    |> Stream.take(1000)

    # Inject mock into download function for testing
    result = CheckpointDownload.download_from_stream(mock_stream, path)

    assert :ok = result
    assert File.stat!(path).size == 1024 * 1000
    end

    @tag :tmp_dir
    test "reports progress during download", %{tmp_dir: tmp_dir} do
    path = Path.join(tmp_dir, "progress_checkpoint.bin")
    total_size = 10_000

    {:ok, progress_agent} = Agent.start_link(fn -> [] end)

    mock_stream = Stream.repeatedly(fn -> :crypto.strong_rand_bytes(1000) end)
                    |> Stream.take(10)

    progress_fn = fn written, total ->
        Agent.update(progress_agent, &[{written, total} | &1])
    end

    :ok = CheckpointDownload.download_from_stream(mock_stream, path,
        progress: progress_fn,
        total_size: total_size
    )

    progress = Agent.get(progress_agent, &Enum.reverse/1)

    # Should have 10 progress updates
    assert length(progress) == 10
    # Final update should show completion
    assert List.last(progress) == {total_size, total_size}
    end
end
end

Test File: test/tinkex/api/streaming_test.exs

defmodule Tinkex.API.StreamingTest do
use ExUnit.Case, async: true

alias Tinkex.API.Streaming

describe "get_stream/2" do
    test "returns lazy stream of response chunks" do
    # Using Bypass for HTTP mocking
    bypass = Bypass.open()

    Bypass.expect(bypass, "GET", "/stream", fn conn ->
        conn
        |> Plug.Conn.put_resp_content_type("application/octet-stream")
        |> Plug.Conn.send_chunked(200)
        |> send_chunks(["chunk1", "chunk2", "chunk3"])
    end)

    url = "http://localhost:#{bypass.port}/stream"
    {:ok, stream} = Streaming.get_stream(url, [])

    # Stream should be lazy
    refute is_list(stream)

    # Enumerate to get chunks
    chunks = Enum.to_list(stream)
    assert chunks == ["chunk1", "chunk2", "chunk3"]
    end
end

describe "post_stream/3" do
    test "returns SSE stream for streaming responses" do
    bypass = Bypass.open()

    Bypass.expect(bypass, "POST", "/sse", fn conn ->
        conn
        |> Plug.Conn.put_resp_content_type("text/event-stream")
        |> Plug.Conn.send_chunked(200)
        |> send_sse_events([
        %{event: "message", data: "hello"},
        %{event: "message", data: "world"}
        ])
    end)

    url = "http://localhost:#{bypass.port}/sse"
    {:ok, stream} = Streaming.post_stream(url, %{}, [])

    events = stream
                |> Tinkex.Streaming.SSEBytesDecoder.iter_bytes()
                |> Enum.to_list()

    assert length(events) == 2
    end
end

# Helper functions
defp send_chunks(conn, []), do: conn
defp send_chunks(conn, [chunk | rest]) do
    {:ok, conn} = Plug.Conn.chunk(conn, chunk)
    send_chunks(conn, rest)
end

defp send_sse_events(conn, []), do: conn
defp send_sse_events(conn, [event | rest]) do
    sse = "event: #{event.event}\ndata: #{event.data}\n\n"
    {:ok, conn} = Plug.Conn.chunk(conn, sse)
    send_sse_events(conn, rest)
end
end

Success Criteria

1. Memory Efficiency Tests Pass
- Binary downloads use O(1) memory regardless of file size
- SSE streams process events without buffering entire response
- Large file tests (10MB+) complete with constant memory
2. Streaming Behavior Tests Pass
- Stream implements Enumerable protocol correctly
- Lazy evaluation verified (partial consumption works)
- Progress callbacks receive accurate byte counts
3. SSE Decoder Tests Pass
- Events split across chunks reassemble correctly
- Events yield immediately when complete
- Buffer state preserves partial events
4. Checkpoint Download Tests Pass
- Files download to disk via streaming (not RAM)
- Progress reporting works for large downloads
- Error handling works for network failures
5. Integration Tests Pass
- End-to-end streaming with mock HTTP server
- Future polling uses streaming SSE correctly
6. All Tests Pass
mix test test/tinkex/stream_test.exs
mix test test/tinkex/streaming/
mix test test/tinkex/checkpoint_download_streaming_test.exs
mix test test/tinkex/api/streaming_test.exs
7. Dialyzer Passes
mix dialyzer

Commands to Run

# Run all streaming tests
mix test test/tinkex/stream_test.exs test/tinkex/streaming/ test/tinkex/checkpoint_download_streaming_test.exs test/tinkex/api/streaming_test.exs

# Run with coverage
mix test --cover test/tinkex/stream_test.exs test/tinkex/streaming/

# Check types
mix dialyzer

# Format code
mix format

Implementation Order

1. Red: Write test/tinkex/streaming/binary_stream_test.exs - all tests fail
2. Green: Implement lib/tinkex/streaming/binary_stream.ex - tests pass
3. Refactor: Optimize chunk handling
4. Red: Write test/tinkex/streaming/sse_bytes_decoder_test.exs - all tests fail
5. Green: Implement lib/tinkex/streaming/sse_bytes_decoder.ex - tests pass
6. Refactor: Ensure no memory leaks in buffer
7. Red: Write test/tinkex/stream_test.exs - all tests fail
8. Green: Implement lib/tinkex/stream.ex with Enumerable - tests pass
9. Refactor: Clean up protocol implementation
10. Red: Write test/tinkex/api/streaming_test.exs - all tests fail
11. Green: Implement lib/tinkex/api/streaming.ex with Finch - tests pass
12. Refactor: Error handling
13. Red: Write test/tinkex/checkpoint_download_streaming_test.exs - fail
14. Green: Update lib/tinkex/checkpoint_download.ex to use BinaryStream
15. Refactor: Add progress callbacks
16. Final integration testing and cleanup
