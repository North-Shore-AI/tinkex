Agent Prompt: Implement Custom Loss Training for Tinkex (Gap #1)

Overview

You are implementing the most critical gap in the Tinkex (Elixir) SDK: making forward_backward_custom actually train the model with custom loss functions, matching the Python SDK's behavior.

The Problem: The current Elixir implementation only computes metrics and gradient norms for telemetry. It never sends gradients back to the server, so no actual training occurs. The Python SDK performs
real training by sending computed gradients as weights in a synthetic dataset.

Required Reading (IN ORDER)

Read these files to understand the problem and solution:

1. Gap Analysis Document

./docs/20251127/gaps_06/01_custom_loss_training.md
This contains the detailed analysis of what's missing and a TDD implementation plan.

2. Python Reference Implementation

./tinker/src/tinker/lib/public_interfaces/training_client.py
Critical method: forward_backward_custom_async (lines 363-412). Study:
- How it runs forward pass to get logprobs
- How it wraps logprobs as PyTorch tensors with requires_grad_()
- How it calls loss.backward() to compute gradients
- How it builds linear_loss_data with negative gradients as weights
- How it calls forward_backward_async again to send gradients to server
- How _CombinedAPIFuture merges custom metrics with server response

3. Current Elixir Implementation

./lib/tinkex/training_client.ex
Study:
- forward_backward_custom/4 (lines 408-421) - public API
- handle_call({:forward_backward_custom, ...}) (lines 881-914)
- do_forward_for_custom_loss/3 (lines 1476-1518)
- extract_logprobs_from_outputs/1 (lines 1536-1555) - flattens per-datum structure

4. Current Custom Loss Output Type

./lib/tinkex/types/custom_loss_output.ex
This returns CustomLossOutput - but Python returns ForwardBackwardOutput. This is a type mismatch.

5. Regularizer Pipeline (Current Implementation)

./lib/tinkex/regularizer/pipeline.ex
The current implementation computes loss but doesn't send gradients to server.

6. Supporting Types

./lib/tinkex/types/datum.ex
./lib/tinkex/types/tensor_data.ex
./lib/tinkex/types/forward_backward_output.ex

7. Python Types for Reference

./tinker/src/tinker/types/datum.py
./tinker/src/tinker/types/tensor_data.py
./tinker/src/tinker/types/forward_backward_output.py

The Gap Explained

Python Flow (TRAINS the model):

1. forward_async(data, "cross_entropy") → get logprobs
2. torch.tensor(logprobs).requires_grad_(True) → enable gradients
3. custom_loss_fn(data, logprobs_tensors) → compute loss
4. loss.backward() → compute gradients w.r.t. logprobs
5. Build linear_loss_data with weights = -gradients
6. forward_backward_async(linear_loss_data, "cross_entropy") → SEND TO SERVER
7. Return ForwardBackwardOutput (compatible with optim_step)

Elixir Flow (DOES NOT TRAIN):

1. forward(data, :cross_entropy) → get logprobs
2. Flatten all logprobs into single Nx tensor (LOSES per-datum structure!)
3. custom_loss_fn(data, logprobs) → compute loss
4. Compute gradient norms for telemetry only
5. Return CustomLossOutput (NOT compatible with optim_step)
6. ❌ NEVER sends gradients to server
7. ❌ No training happens

Implementation Requirements

Key Changes Needed:

1. Preserve per-datum logprob structure - Don't flatten. Keep [Nx.Tensor.t()] list matching input data length.
2. Keep dtype/shape metadata when turning loss_fn_outputs into tensors so weights line up with per-datum loss_fn_inputs.
3. Compute gradients using Nx.Defn.grad - Use Nx.Defn.grad/2 to compute gradients w.r.t. logprobs.
4. Build synthetic linear_loss_data - Create new Datum structs with:
- Same model_input as original
- loss_fn_inputs with "target_tokens" from original AND "weights" = -gradient
5. Call forward_backward with synthetic data - Send gradients to server via the existing forward_backward API.
6. Return ForwardBackwardOutput - Not CustomLossOutput. Must be compatible with optim_step.
7. Merge custom metrics - Add metrics from custom loss to server response.
8. Advance request/seq IDs when sending forward chunks (don’t discard the updated counter from allocate_request_ids/2).

TDD Implementation Plan

Phase 1: Test Infrastructure

Create test file: ./test/tinkex/training/custom_loss_test.exs

defmodule Tinkex.Training.CustomLossTest do
use ExUnit.Case, async: true

alias Tinkex.Training.CustomLoss
alias Tinkex.Types.{Datum, TensorData, ForwardBackwardOutput}

describe "extract_per_datum_logprobs/1" do
    test "preserves per-datum structure from forward output" do
    # Setup: Create ForwardBackwardOutput with 3 datums
    output = %ForwardBackwardOutput{
        loss_fn_output_type: "cross_entropy",
        loss_fn_outputs: [
        %{"logprobs" => %{"data" => [-1.0, -2.0], "dtype" => "float32"}},
        %{"logprobs" => %{"data" => [-0.5, -1.5, -2.5], "dtype" => "float32"}},
        %{"logprobs" => %{"data" => [-3.0], "dtype" => "float32"}}
        ],
        metrics: %{"loss" => 1.5}
    }

    {:ok, logprobs_list} = CustomLoss.extract_per_datum_logprobs(output)

    assert length(logprobs_list) == 3
    assert Nx.shape(Enum.at(logprobs_list, 0)) == {2}
    assert Nx.shape(Enum.at(logprobs_list, 1)) == {3}
    assert Nx.shape(Enum.at(logprobs_list, 2)) == {1}
    end
end

describe "compute_gradients/2" do
    test "computes gradients for each logprobs tensor" do
    logprobs_list = [
        Nx.tensor([-1.0, -2.0]),
        Nx.tensor([-0.5, -1.5])
    ]

    loss_fn = fn _data, logprobs_list ->
        total = logprobs_list |> Enum.map(&Nx.sum/1) |> Enum.reduce(&Nx.add/2)
        {total, %{"custom" => 1.0}}
    end

    {:ok, gradients} = CustomLoss.compute_gradients([], logprobs_list, loss_fn)

    assert length(gradients) == 2
    # Gradient of sum is 1 for each element
    assert Nx.to_flat_list(Enum.at(gradients, 0)) == [1.0, 1.0]
    end
end

describe "build_linear_loss_data/3" do
    test "creates synthetic data with negative gradients as weights" do
    original_data = [
        %Datum{
        model_input: %{chunks: [%{data: [1, 2, 3]}]},
        loss_fn_inputs: %{"target_tokens" => %TensorData{data: [4, 5, 6], dtype: :int64}}
        }
    ]

    gradients = [Nx.tensor([0.1, 0.2, 0.3])]

    linear_data = CustomLoss.build_linear_loss_data(original_data, gradients)

    assert length(linear_data) == 1
    datum = hd(linear_data)

    # Same model_input
    assert datum.model_input == hd(original_data).model_input

    # Has target_tokens from original
    assert datum.loss_fn_inputs["target_tokens"] == hd(original_data).loss_fn_inputs["target_tokens"]

    # Has weights = -gradient
    weights = datum.loss_fn_inputs["weights"]
    assert weights.data == [-0.1, -0.2, -0.3]
    end
end

describe "forward_backward_custom integration" do
    @tag :integration
    test "returns ForwardBackwardOutput compatible with optim_step" do
    # This test requires a mock or actual server
    # The key assertion: result type is ForwardBackwardOutput, not CustomLossOutput
    end
end
end

Phase 2: Implement Core Module

Create: ./lib/tinkex/training/custom_loss.ex

defmodule Tinkex.Training.CustomLoss do
@moduledoc """
Custom loss training that actually sends gradients to the server.

This module implements the gradient computation and synthetic dataset
construction needed to train with custom loss functions, matching
the Python SDK's `forward_backward_custom_async` behavior.
"""

alias Tinkex.Types.{Datum, TensorData, ForwardBackwardOutput}

@doc """
Extract per-datum logprobs preserving structure.

Unlike the flattening approach, this returns a list of Nx tensors
where each tensor corresponds to one input datum.
"""
@spec extract_per_datum_logprobs(ForwardBackwardOutput.t()) ::
    {:ok, [Nx.Tensor.t()]} | {:error, term()}
def extract_per_datum_logprobs(%ForwardBackwardOutput{loss_fn_outputs: outputs}) do
    # Implementation here
end

@doc """
Compute gradients of loss w.r.t. each logprobs tensor.

Uses Nx.Defn.grad to compute gradients, similar to PyTorch's .backward()
"""
@spec compute_gradients(list(), [Nx.Tensor.t()], function()) ::
    {:ok, [Nx.Tensor.t()]} | {:error, term()}
def compute_gradients(data, logprobs_list, loss_fn) do
    # Implementation here - use Nx.Defn.grad
end

@doc """
Build synthetic dataset with gradients as weights.

Each datum gets:
- Same model_input as original
- target_tokens from original
- weights = -gradient (negative because we minimize loss)
"""
@spec build_linear_loss_data([Datum.t()], [Nx.Tensor.t()]) :: [Datum.t()]
def build_linear_loss_data(original_data, gradients) do
    # Implementation here
end
end

Phase 3: Update TrainingClient

Modify ./lib/tinkex/training_client.ex:

1. Change handle_call({:forward_backward_custom, ...}) to:
- Call forward to get logprobs (preserving per-datum structure)
- Compute gradients using Nx.Defn.grad
- Build synthetic linear_loss_data
- Call forward_backward with synthetic data
- Merge custom metrics into response
- Return ForwardBackwardOutput (not CustomLossOutput)
2. Update forward_backward_custom/4 spec to return ForwardBackwardOutput.t()
3. Ensure request_id_counter/seq_ids are incremented when issuing forward and backward requests (fix the dropped counter in do_forward_for_custom_loss/3).

Phase 4: Integration Tests

Create integration tests that verify:
1. Custom loss training actually updates model weights
2. optim_step works after forward_backward_custom
3. Metrics are properly merged
4. Error handling for invalid loss functions

Critical Implementation Details

Nx.Defn.grad Usage

# Computing gradient of loss w.r.t. logprobs
defn compute_loss_gradient(logprobs, loss_fn_captured) do
# loss_fn must be defn-compatible or use Nx.Defn.grad/3 with fun
grad(logprobs, fn lp ->
    # Your loss computation here
    Nx.sum(lp)  # Example
end)
end

Handling Variable-Length Logprobs

Python handles variable-length by passing negative gradients as the weights key in loss_fn_inputs. The server knows to apply these element-wise during the "cross_entropy" backward pass.

Type Compatibility

The function MUST return {:ok, ForwardBackwardOutput.t()} so callers can:
{:ok, task} = TrainingClient.forward_backward_custom(client, data, &my_loss/2)
{:ok, fwdbwd_output} = Task.await(task)

# This must work:
{:ok, optim_task} = TrainingClient.optim_step(client, adam_params)

Success Criteria

1. ✅ All new tests pass
2. ✅ forward_backward_custom returns ForwardBackwardOutput
3. ✅ Gradients are sent to server via synthetic forward_backward call
4. ✅ optim_step works after custom loss training
5. ✅ Custom metrics are preserved in output
6. ✅ No breaking changes to existing API
7. ✅ Per-datum structure is preserved (not flattened)

Files to Create/Modify

Create:

- ./lib/tinkex/training/custom_loss.ex - Core gradient computation
- ./test/tinkex/training/custom_loss_test.exs - Unit tests

Modify:

- ./lib/tinkex/training_client.ex - Update forward_backward_custom implementation
- ./test/tinkex/training_client_test.exs - Add integration tests

Commands

# Run tests
mix test test/tinkex/training/custom_loss_test.exs

# Run all tests
mix test

# Format code
mix format

# Check types (if dialyzer configured)
mix dialyzer

Do NOT:

- Change the public API signature of forward_backward_custom/4
- Remove CustomLossOutput type (keep for backward compat, but don't return it from training)
- Flatten logprobs into a single tensor (must preserve per-datum structure)
- Skip the synthetic forward_backward call (that's the whole point!)
- Reuse the same seq_id/request_id across forwards (must advance the counter)
