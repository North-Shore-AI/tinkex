Agent Prompt: Implement Queue State Observer in Clients for Tinkex (Gap #4)

Overview

You are implementing queue state observer integration in SamplingClient and TrainingClient for the Tinkex (Elixir) SDK. The observer infrastructure exists (Tinkex.QueueStateObserver behaviour,
Tinkex.Future telemetry), but the clients don't implement the behaviour or pass themselves as observers.

The Problem: In Python, both SamplingClient and TrainingClient implement QueueStateObserver and automatically log human-friendly messages when queue state changes (e.g., "concurrent LoRA rate limit
hit"). In Elixir, the behaviour exists and Future.poll/2 supports it, but the clients don't implement or use it - they only pass through external observers. Python only invokes observers on 408
responses (queue_state inside the 200 try_again body is ignored); Elixir fires on TryAgainResponse transitions.

Required Reading (IN ORDER)

Read these files to understand the problem and solution:

1. Gap Analysis Document

./docs/20251127/gaps_06/04_queue_state_observer.md
This contains the detailed analysis of what's missing (including the 408-only observer trigger in Python), parsing differences, and a TDD implementation plan.

2. Python Implementation - QueueState Enum & Observer ABC

./tinker/src/tinker/lib/api_future_impl.py
Lines 38-48 - Core definitions:
class QueueState(Enum):
    ACTIVE = "active"
    PAUSED_RATE_LIMIT = "paused_rate_limit"
    PAUSED_CAPACITY = "paused_capacity"
    UNKNOWN = "unknown"

class QueueStateObserver(ABC):
    @abstractmethod
    def on_queue_state_change(self, queue_state: QueueState) -> None:
        raise NotImplementedError

Lines 148-163 - Observer callback on 408 errors:
if e.status_code == 408:
    if self._queue_state_observer is not None:
        with contextlib.suppress(Exception):
            response = e.response.json()
            if queue_state_str := response.get("queue_state", None):
                # Parse and notify observer
                self._queue_state_observer.on_queue_state_change(queue_state)
    continue
Note: queue_state in 200 "try_again" responses is ignored in Python; only 408 JSON bodies trigger the observer.

3. Python SamplingClient Observer Implementation

./tinker/src/tinker/lib/public_interfaces/sampling_client.py
Line 33 - Class declaration:
class SamplingClient(TelemetryProvider, QueueStateObserver):

Lines 301-317 - Observer implementation:
def on_queue_state_change(self, queue_state: QueueState) -> None:
    QUEUE_STATE_LOG_INTERVAL = 60
    if queue_state == QueueState.ACTIVE:
        return
    if time.time() - self._last_queue_state_logged < QUEUE_STATE_LOG_INTERVAL:
        return
    if queue_state == QueueState.PAUSED_RATE_LIMIT:
        reason = "concurrent LoRA rate limit hit"
    elif queue_state == QueueState.PAUSED_CAPACITY:
        reason = "out of capacity"
    else:
        reason = "unknown"
    self._last_queue_state_logged = time.time()
    logger.warning(f"Sampling is paused for sampler {self._sampling_session_id}. Reason: {reason}")

4. Python TrainingClient Observer Implementation

./tinker/src/tinker/lib/public_interfaces/training_client.py
Line 50 - Class declaration:
class TrainingClient(TelemetryProvider, QueueStateObserver):

Lines 833-847 - Observer implementation:
def on_queue_state_change(self, queue_state: QueueState) -> None:
    QUEUE_STATE_LOG_INTERVAL = 60
    if queue_state == QueueState.ACTIVE:
        return
    if time.time() - self._last_queue_state_logged < QUEUE_STATE_LOG_INTERVAL:
        return
    self._last_queue_state_logged = time.time()
    if queue_state == QueueState.PAUSED_RATE_LIMIT:
        reason = "concurrent models rate limit hit"
    elif queue_state == QueueState.PAUSED_CAPACITY:
        reason = "out of capacity"
    else:
        reason = "unknown"
    logger.warning(f"Training is paused for {self.model_id}. Reason: {reason}")

5. Current Elixir Observer Infrastructure

./lib/tinkex/queue_state_observer.ex
The behaviour definition (already exists):
@callback on_queue_state_change(QueueState.t()) :: any()

./lib/tinkex/types/queue_state.ex
The QueueState type (already exists):
@type t :: :active | :paused_rate_limit | :paused_capacity | :unknown

./lib/tinkex/future.ex
Future module that already:
- Emits [:tinkex, :queue, :state_change] telemetry
- Accepts :queue_state_observer option
- Calls observer.on_queue_state_change(queue_state) on transitions
- Deduplicates by state change (not time-based) and fires on TryAgainResponse, not 408

6. Current Elixir Clients (Missing Observer Implementation)

./lib/tinkex/sampling_client.ex
./lib/tinkex/training_client.ex
Neither implements @behaviour Tinkex.QueueStateObserver.

The Gap Explained

Python Flow (Automatic Logging):

# SamplingClient/TrainingClient implement QueueStateObserver
# When _APIFuture gets a 408 with queue_state, it calls:
self._queue_state_observer.on_queue_state_change(queue_state)

# Client logs human-friendly message (with debouncing):
# "Sampling is paused for sampler ABC123. Reason: concurrent LoRA rate limit hit"
# "Training is paused for model-xyz. Reason: out of capacity"

Current Elixir Flow (Silent):

# Future.poll/2 emits telemetry and notifies observer IF provided
# But SamplingClient/TrainingClient:
# 1. Don't implement @behaviour Tinkex.QueueStateObserver
# 2. Don't pass themselves as :queue_state_observer to Future.poll
# 3. Only pass through external observers from opts

# Result: No automatic logging, silent queue state changes
# Also, Elixir's QueueState.parse/1 is case-insensitive and defaults to :unknown; Python's manual parsing is case-sensitive and defaults to UNKNOWN on mismatch.

Target Elixir Flow (Automatic Logging):

# SamplingClient implements QueueStateObserver, passes self to poll opts
# When queue state changes:
# Logger.warning("Sampling is paused for session #{session_id}. Reason: #{reason}")

# Same for TrainingClient:
# Logger.warning("Training is paused for model #{model_id}. Reason: #{reason}")

Implementation Requirements

Modify SamplingClient

defmodule Tinkex.SamplingClient do
use GenServer
use Tinkex.Telemetry.Provider

@behaviour Tinkex.QueueStateObserver  # ADD THIS

# Add to state struct:
# last_queue_state_logged: 0 (timestamp in seconds)

# Implement the callback:
@impl Tinkex.QueueStateObserver
def on_queue_state_change(queue_state) do
    # Implementation with debouncing
end

# In places that call Future.poll, pass the observer:
# queue_state_observer: __MODULE__  # or pid-based
end

Modify TrainingClient

defmodule Tinkex.TrainingClient do
use GenServer
use Tinkex.Telemetry.Provider

@behaviour Tinkex.QueueStateObserver  # ADD THIS

# Add to state struct:
# last_queue_state_logged: 0

# Implement the callback:
@impl Tinkex.QueueStateObserver
def on_queue_state_change(queue_state) do
    # Implementation with debouncing
end
end

TDD Implementation Plan

Phase 1: Create Shared Queue State Logger Module

Create test/tinkex/queue_state_logger_test.exs:

defmodule Tinkex.QueueStateLoggerTest do
use ExUnit.Case, async: true

import ExUnit.CaptureLog

alias Tinkex.QueueStateLogger
alias Tinkex.Types.QueueState

describe "log_state_change/3" do
    test "logs warning for paused_rate_limit" do
    log = capture_log(fn ->
        QueueStateLogger.log_state_change(:paused_rate_limit, "sampling", "session-123")
    end)

    assert log =~ "Sampling is paused"
    assert log =~ "session-123"
    assert log =~ "concurrent LoRA rate limit hit"
    end

    test "logs warning for paused_capacity" do
    log = capture_log(fn ->
        QueueStateLogger.log_state_change(:paused_capacity, "training", "model-abc")
    end)

    assert log =~ "Training is paused"
    assert log =~ "model-abc"
    assert log =~ "out of capacity"
    end

    test "does not log for active state" do
    log = capture_log(fn ->
        QueueStateLogger.log_state_change(:active, "sampling", "session-123")
    end)

    assert log == ""
    end

    test "logs unknown reason for unknown state" do
    log = capture_log(fn ->
        QueueStateLogger.log_state_change(:unknown, "training", "model-xyz")
    end)

    assert log =~ "unknown"
    end
end

describe "should_log?/2 (debouncing)" do
    test "returns true when enough time has passed" do
    last_logged = System.system_time(:second) - 61
    assert QueueStateLogger.should_log?(last_logged, 60) == true
    end

    test "returns false within debounce interval" do
    last_logged = System.system_time(:second) - 30
    assert QueueStateLogger.should_log?(last_logged, 60) == false
    end

    test "returns true for initial log (0 timestamp)" do
    assert QueueStateLogger.should_log?(0, 60) == true
    end
end

describe "reason_for_state/2" do
    test "returns LoRA message for sampling" do
    assert QueueStateLogger.reason_for_state(:paused_rate_limit, :sampling) ==
                "concurrent LoRA rate limit hit"
    end

    test "returns models message for training" do
    assert QueueStateLogger.reason_for_state(:paused_rate_limit, :training) ==
                "concurrent models rate limit hit"
    end

    test "returns capacity message for both" do
    assert QueueStateLogger.reason_for_state(:paused_capacity, :sampling) == "out of capacity"
    assert QueueStateLogger.reason_for_state(:paused_capacity, :training) == "out of capacity"
    end
end
end

Phase 2: Test SamplingClient Observer

Create/update test/tinkex/sampling_client_observer_test.exs:

defmodule Tinkex.SamplingClientObserverTest do
use ExUnit.Case, async: false

import ExUnit.CaptureLog

alias Tinkex.SamplingClient

describe "QueueStateObserver implementation" do
    test "SamplingClient implements the behaviour" do
    behaviours = SamplingClient.__info__(:attributes)[:behaviour] || []
    assert Tinkex.QueueStateObserver in behaviours
    end

    test "on_queue_state_change/1 is defined" do
    assert function_exported?(SamplingClient, :on_queue_state_change, 1)
    end
end

describe "on_queue_state_change/1" do
    setup do
    # Start a mock SamplingClient or use process dictionary for state
    {:ok, %{}}
    end

    test "logs warning for paused_rate_limit with session ID" do
    # This requires a running client to get session_id context
    # Or we can test the module function directly
    end

    test "respects 60-second debounce interval" do
    # First call should log
    # Second call within 60s should not log
    # Call after 60s should log again
    end

    test "does not log for :active state" do
    log = capture_log(fn ->
        SamplingClient.on_queue_state_change(:active)
    end)

    assert log == ""
    end
end
end

Phase 3: Test TrainingClient Observer

Create/update test/tinkex/training_client_observer_test.exs:

defmodule Tinkex.TrainingClientObserverTest do
use ExUnit.Case, async: false

import ExUnit.CaptureLog

alias Tinkex.TrainingClient

describe "QueueStateObserver implementation" do
    test "TrainingClient implements the behaviour" do
    behaviours = TrainingClient.__info__(:attributes)[:behaviour] || []
    assert Tinkex.QueueStateObserver in behaviours
    end

    test "on_queue_state_change/1 is defined" do
    assert function_exported?(TrainingClient, :on_queue_state_change, 1)
    end
end

describe "on_queue_state_change/1" do
    test "logs warning for paused_rate_limit with model ID" do
    # Test that message includes model_id and correct reason
    end

    test "uses 'concurrent models rate limit hit' for training" do
    log = capture_log(fn ->
        # Trigger the observer
    end)

    assert log =~ "concurrent models rate limit hit"
    end
end
end

Phase 4: Integration Test - Observer Wiring

defmodule Tinkex.QueueStateIntegrationTest do
use ExUnit.Case, async: false

import ExUnit.CaptureLog

describe "Future.poll with client as observer" do
    @tag :integration
    test "SamplingClient receives queue state callbacks" do
    # 1. Start SamplingClient
    # 2. Mock server to return TryAgainResponse with queue_state
    # 3. Call sample/4 which triggers Future.poll
    # 4. Verify log output contains expected message
    end

    @tag :integration
    test "TrainingClient receives queue state callbacks" do
    # 1. Start TrainingClient
    # 2. Mock server to return TryAgainResponse with queue_state
    # 3. Call forward_backward which triggers Future.poll
    # 4. Verify log output contains expected message
    end
end
end

Implementation Details

Shared Logger Module

Create ./lib/tinkex/queue_state_logger.ex:

defmodule Tinkex.QueueStateLogger do
@moduledoc """
Shared logging utilities for queue state changes.

Provides human-readable messages matching Python SDK behavior,
with debouncing to avoid log spam.
"""

require Logger

@log_interval_seconds 60

@doc """
Log a queue state change with appropriate human-readable reason.

## Parameters
- queue_state: :active | :paused_rate_limit | :paused_capacity | :unknown
- client_type: :sampling | :training
- identifier: session_id for sampling, model_id for training
"""
@spec log_state_change(atom(), atom(), String.t()) :: :ok
def log_state_change(:active, _client_type, _identifier), do: :ok

def log_state_change(queue_state, client_type, identifier) do
    reason = reason_for_state(queue_state, client_type)
    client_name = client_type_name(client_type)

    Logger.warning("#{client_name} is paused for #{identifier}. Reason: #{reason}")
end

@doc """
Check if enough time has passed since last log.
"""
@spec should_log?(integer(), integer()) :: boolean()
def should_log?(last_logged, interval \\ @log_interval_seconds) do
    System.system_time(:second) - last_logged >= interval
end

@doc """
Get human-readable reason for queue state.
"""
@spec reason_for_state(atom(), atom()) :: String.t()
def reason_for_state(:paused_rate_limit, :sampling), do: "concurrent LoRA rate limit hit"
def reason_for_state(:paused_rate_limit, :training), do: "concurrent models rate limit hit"
def reason_for_state(:paused_capacity, _), do: "out of capacity"
def reason_for_state(_, _), do: "unknown"

defp client_type_name(:sampling), do: "Sampling"
defp client_type_name(:training), do: "Training"
end

SamplingClient Changes

In ./lib/tinkex/sampling_client.ex:

defmodule Tinkex.SamplingClient do
use GenServer
use Tinkex.Telemetry.Provider

@behaviour Tinkex.QueueStateObserver  # ADD

# In init, add to state:
# last_queue_state_logged: 0

# Add the callback implementation:
@impl Tinkex.QueueStateObserver
def on_queue_state_change(queue_state) do
    # Note: This is a module callback, need to get state from process
    # Option 1: Use process dictionary
    # Option 2: Use ETS (already have SamplingRegistry)
    # Option 3: GenServer.cast to self

    # For simplicity, we can use the process dictionary or ETS
    do_queue_state_change(queue_state)
end

defp do_queue_state_change(:active), do: :ok

defp do_queue_state_change(queue_state) do
    alias Tinkex.QueueStateLogger

    # Get state from process dict or ETS
    last_logged = get_last_queue_state_logged()

    if QueueStateLogger.should_log?(last_logged) do
    session_id = get_session_id()
    QueueStateLogger.log_state_change(queue_state, :sampling, session_id)
    put_last_queue_state_logged(System.system_time(:second))
    end

    :ok
end

# In poll_opts or wherever Future.poll is called, add:
# queue_state_observer: __MODULE__
end

TrainingClient Changes

In ./lib/tinkex/training_client.ex:

defmodule Tinkex.TrainingClient do
use GenServer
use Tinkex.Telemetry.Provider

@behaviour Tinkex.QueueStateObserver  # ADD

# Add to state struct: last_queue_state_logged: 0

@impl Tinkex.QueueStateObserver
def on_queue_state_change(queue_state) do
    do_queue_state_change(queue_state)
end

defp do_queue_state_change(:active), do: :ok

defp do_queue_state_change(queue_state) do
    alias Tinkex.QueueStateLogger

    last_logged = get_last_queue_state_logged()

    if QueueStateLogger.should_log?(last_logged) do
    model_id = get_model_id()
    QueueStateLogger.log_state_change(queue_state, :training, model_id)
    put_last_queue_state_logged(System.system_time(:second))
    end

    :ok
end

# In poll_opts_with_type or similar, add:
# queue_state_observer: __MODULE__
end

State Access Pattern

Since the callback is module-level but needs instance state, options include:

1. Process Dictionary - Store last_queue_state_logged in process dict during GenServer execution
2. ETS - TrainingClient already has state; SamplingClient uses SamplingRegistry
3. Module Attribute Agent - Less clean but works

Recommended: Use process dictionary since callbacks happen within the GenServer's Task processes.

Files to Create

lib/tinkex/queue_state_logger.ex
test/tinkex/queue_state_logger_test.exs
test/tinkex/sampling_client_observer_test.exs
test/tinkex/training_client_observer_test.exs

Files to Modify

lib/tinkex/sampling_client.ex
lib/tinkex/training_client.ex

Success Criteria

1. ✅ All new tests pass
2. ✅ SamplingClient implements @behaviour Tinkex.QueueStateObserver
3. ✅ TrainingClient implements @behaviour Tinkex.QueueStateObserver
4. ✅ Queue state changes trigger human-readable log warnings
5. ✅ Logs are debounced (60-second interval)
6. ✅ SamplingClient uses "concurrent LoRA rate limit hit" message
7. ✅ TrainingClient uses "concurrent models rate limit hit" message
8. ✅ :active state changes don't log
9. ✅ Clients pass themselves as :queue_state_observer to Future.poll
10. ✅ Telemetry still works (additive, not replacing)
11. ✅ No breaking changes to existing API

Commands

# Run queue state logger tests
mix test test/tinkex/queue_state_logger_test.exs

# Run observer tests
mix test test/tinkex/sampling_client_observer_test.exs
mix test test/tinkex/training_client_observer_test.exs

# Run all tests
mix test

# Format code
mix format

# Compile with warnings
mix compile --warnings-as-errors

Do NOT:

- Remove existing telemetry events (keep both telemetry + observer)
- Change the QueueStateObserver behaviour signature
- Add external dependencies for logging
- Log :active state transitions
- Log more frequently than every 60 seconds per state
- Break existing code that passes custom :queue_state_observer in opts
