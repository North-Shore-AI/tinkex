# Structured Regularizers for Tinkex

**Port of Python SDK Feature to Elixir**

---

## Overview

This documentation describes the design and implementation of structured regularizer composition for the Tinkex Elixir SDK, porting the Python SDK feature from commit `22e6fc9b4f85c7dbb07e72aa415a26fb454ef504`.

## Documentation Index

### Core Documents

| Document | Description |
|----------|-------------|
| [00_TECHNICAL_SPECIFICATION.md](./00_TECHNICAL_SPECIFICATION.md) | Complete technical specification including requirements, architecture, types, API, and implementation strategy |
| [01_IMPLEMENTATION_GUIDE.md](./01_IMPLEMENTATION_GUIDE.md) | Step-by-step implementation guide with code examples and testing patterns |

### Architecture Decision Records (ADRs)

| ADR | Title | Status |
|-----|-------|--------|
| [ADR-001](./adr/ADR-001_REGULARIZER_ARCHITECTURE.md) | Regularizer Architecture | Proposed |
| [ADR-002](./adr/ADR-002_CONCURRENCY_MODEL.md) | Concurrency Model | Proposed |
| [ADR-003](./adr/ADR-003_TELEMETRY_SCHEMA.md) | Telemetry Schema | Proposed |

## Feature Summary

### What This Feature Provides

1. **Structured Regularizer Composition**
   - Define multiple named regularizers with independent weights
   - Automatic composition: `total_loss = base_loss + Σ(weight_i × reg_i)`
   - Per-regularizer metrics collection

2. **Gradient Magnitude Tracking**
   - Compute L2 gradient norms per regularizer
   - Monitor which regularizers dominate training signal
   - Optional via `track_grad_norms: true`

3. **Process-Based Parallel Execution**
   - Leverage BEAM schedulers for true parallelism
   - `Task.async_stream` for concurrent regularizer execution
   - Async regularizer support for I/O-bound operations

4. **Structured Telemetry**
   - Erlang `:telemetry` events at multiple granularities
   - Metrics schema compatible with Python SDK
   - JSON serialization via Jason

## Quick Start

```elixir
# Define base loss function
def base_cross_entropy(_data, logprobs) do
  loss = Nx.negate(Nx.mean(logprobs))
  {loss, %{"mean_nll" => Nx.to_number(loss)}}
end

# Define regularizers
regularizers = [
  %Tinkex.Types.RegularizerSpec{
    fn: fn _data, logprobs ->
      l1 = Nx.sum(Nx.abs(logprobs))
      {l1, %{"l1_total" => Nx.to_number(l1)}}
    end,
    weight: 0.01,
    name: "l1_sparsity"
  }
]

# Execute custom loss with regularizers
{:ok, task} = Tinkex.TrainingClient.forward_backward_custom(
  training_client,
  data,
  &base_cross_entropy/2,
  regularizers: regularizers,
  track_grad_norms: true
)

{:ok, output} = Task.await(task, 60_000)

# Access structured metrics
IO.puts("Total loss: #{output.loss_total}")
IO.puts("Regularizer total: #{output.regularizer_total}")
IO.puts("L1 contribution: #{output.regularizers["l1_sparsity"].contribution}")
```

## Key Differences from Python SDK

| Aspect | Python | Elixir |
|--------|--------|--------|
| Async model | `asyncio` + `ThreadPoolExecutor` | `Task` + BEAM schedulers |
| Tensor library | PyTorch | Nx/EXLA |
| Autodiff | `torch.autograd.grad()` | `Nx.Defn.grad()` |
| Type system | TypedDict (runtime) | @type specs (dialyzer) |
| Parallelism | GIL-limited | True parallel processes |
| Thread pool option | `run_sync_in_executor` | Not needed (BEAM handles it) |

## Implementation Phases

| Phase | Description | Estimated Effort |
|-------|-------------|------------------|
| 1 | Type definitions | 2 days |
| 2 | Regularizer behaviour | 1 day |
| 3 | Gradient tracker (Nx) | 3 days |
| 4 | Executor (Task parallelism) | 2 days |
| 5 | Pipeline orchestration | 2 days |
| 6 | TrainingClient integration | 2 days |
| 7 | Telemetry | 1 day |
| 8 | Documentation & examples | 2 days |

## Related Resources

### Source Materials

- **Python SDK Commit**: `22e6fc9b4f85c7dbb07e72aa415a26fb454ef504`
- **Original Specification**: See [00_TECHNICAL_SPECIFICATION.md](./00_TECHNICAL_SPECIFICATION.md) Section 2

### External Documentation

- [Nx Documentation](https://hexdocs.pm/nx)
- [EXLA Backend](https://hexdocs.pm/exla)
- [Elixir Task](https://hexdocs.pm/elixir/Task.html)
- [Erlang Telemetry](https://hexdocs.pm/telemetry)

### Tinkex Codebase

Key files to understand before implementation:

```
lib/tinkex/
├── training_client.ex      # GenServer to modify
├── types/
│   ├── datum.ex            # Training data structure
│   ├── tensor_data.ex      # Nx conversion
│   └── forward_backward_output.ex
├── future.ex               # Async polling pattern
└── telemetry.ex            # Existing telemetry
```

## Open Questions

1. Should gradient accumulation across batches be supported?
2. How to handle regularizer state (e.g., EMA tracking)?
3. Distributed cluster behavior?
4. Built-in regularizer library (L1, L2, entropy)?

## Authors

- Generated by Claude Code, November 25, 2025
- Based on Python SDK work by nshkrdotcom

---

*This documentation is part of the North-Shore-AI/tinkex project.*
